# -*- coding: utf-8 -*-
"""Next Word Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10j03JxMiBKF7cAelmW2qY-EY3sVM7-7D
"""

import numpy as np
from nltk.tokenize import RegexpTokenizer
from keras.models import Sequential, load_model
from keras.layers import LSTM
from keras.layers.core import Dense, Activation
from tensorflow.keras.optimizers import RMSprop
import matplotlib.pyplot as plt
import pickle
import heapq

from google.colab import files
uploaded = files.upload()

text = open('Cover Letter.txt', encoding='UTF-8').read().lower()
print('Corpus Length:',len(text))

tokenizer = RegexpTokenizer(r'\w+')
words = tokenizer.tokenize(text)

unique_words = np.unique(words)
unique_words_index = dict((c,i)for i, c in enumerate(unique_words))

WORD_LENGTH = 5
prev_words = []
next_words = []
for i in range(len(words) - WORD_LENGTH):
  prev_words.append(words[i:i + WORD_LENGTH])
  next_words.append(words[i + WORD_LENGTH])
print(prev_words[0])
print(next_words[0])

X = np.zeros((len(prev_words),WORD_LENGTH, len(unique_words)), dtype = bool)
Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)
for i, each_words in enumerate(prev_words):
  for j,each_word in enumerate(each_words):
      X[i,j, unique_words_index[each_word]] = 1
  Y[i, unique_words_index[next_words[i]]] = 1

print(X[0][0])

model = Sequential()
model.add(LSTM(128, input_shape=(WORD_LENGTH,len(unique_words))))
model.add(Dense(len(unique_words)))
model.add(Activation('softmax'))

optimizer = RMSprop(lr=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=2, shuffle=True).history

model.save('keras_next_word_model.h5')
pickle.dump(history, open("history.p","wb"))
model = load_model('keras_next_word_model.h5')
history = pickle.load(open("history.p","rb"))

def prepare_input(text):
  x=np.zeros((1,WORD_LENGTH,len(unique_words)))
  for t, word in enumerate(text.split()):
    print(word)
    x[0, t, unique_words_index[word]] = 1
  return x
prepare_input("I possess all the skills".lower())

def sample(preds, top_n=3):
  preds = np.asarray(preds).astype('float64')
  preds = np.log(preds)
  exp_preds = np.exp(preds)
  preds = exp_preds / np.sum(exp_preds)

  return heapq.nlargest(top_n, range(len(preds)), preds.take)

def predict_completion(text, n=3):
  if text =="":
    return("0")
  x = prepare_input(text)
  preds = model.predict(x, verbose=0)[0]
  next_indices = sample(preds, n)
  return[unique_words[idx] for idx in next_indices]

m = "I'm pretty confident that I am the best candidate for this job role"
print("correct sentences: ",m)
seq = " ".join(tokenizer.tokenize(m.lower())[0:5])
print("Sequence: ",seq)
print("Next possible word: ", predict_completion(seq, 5))